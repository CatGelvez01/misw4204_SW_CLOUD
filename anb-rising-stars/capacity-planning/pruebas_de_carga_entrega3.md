# PRUEBAS DE CARGA - ENTREGA 3
## An√°lisis de Capacidad y Escalabilidad

**Fecha de Ejecuci√≥n**: [Completar con fecha de pruebas]
**Responsables**: [Completar con nombres del equipo]
**Versi√≥n**: 1.0

---

## 1. Introducci√≥n

Este documento presenta el an√°lisis detallado de las pruebas de carga realizadas en la Entrega 3, evaluando la capacidad de escalabilidad de la aplicaci√≥n web desplegada en AWS con implementaci√≥n de Load Balancer, Auto Scaling y almacenamiento en S3.

## 2. Objetivos de las Pruebas

- Validar el funcionamiento del Load Balancer en distribuci√≥n de tr√°fico
- Evaluar las pol√≠ticas de Auto Scaling bajo diferentes cargas
- Medir el rendimiento de la aplicaci√≥n con m√∫ltiples instancias
- Identificar cuellos de botella en la arquitectura
- Determinar la capacidad m√°xima de usuarios concurrentes
- Validar la integraci√≥n con S3 bajo carga

## 3. Escenarios de Prueba

### Escenario 1: Carga Progresiva (Ramp-up)

**Objetivo**: Evaluar el comportamiento del sistema bajo aumento gradual de usuarios

**Configuraci√≥n**:
- Usuarios iniciales: 10
- Incremento: 10 usuarios cada 2 minutos
- Usuarios m√°ximos: 100
- Duraci√≥n total: 20 minutos
- Endpoints probados: GET /videos, POST /upload, GET /health

**M√©tricas a Recolectar**:
- Tiempo de respuesta promedio
- Percentil 95 y 99 de latencia
- Tasa de error (%)
- Throughput (req/s)
- CPU de instancias
- N√∫mero de instancias activas
- Tiempo de escalado

**Criterios de √âxito**:
- Tiempo de respuesta < 2 segundos (p95)
- Tasa de error < 1%
- Escalado autom√°tico en < 3 minutos
- M√°ximo 3 instancias activas

### Escenario 2: Carga Sostenida (Steady State)

**Objetivo**: Evaluar la estabilidad del sistema bajo carga constante

**Configuraci√≥n**:
- Usuarios concurrentes: 50
- Duraci√≥n: 30 minutos
- Mix de operaciones:
  - 40% GET /videos
  - 30% POST /upload (con video peque√±o)
  - 20% GET /video/{id}
  - 10% DELETE /video/{id}

**M√©tricas a Recolectar**:
- Tiempo de respuesta promedio y desviaci√≥n est√°ndar
- Percentiles de latencia (p50, p95, p99)
- Tasa de error
- Throughput sostenido
- Utilizaci√≥n de recursos (CPU, memoria, disco)
- Comportamiento de Auto Scaling
- Latencia de S3

**Criterios de √âxito**:
- Tiempo de respuesta estable
- Tasa de error < 0.5%
- N√∫mero de instancias estable (1-2)
- Sin degradaci√≥n de rendimiento

## 4. Herramientas de Prueba

- **Apache JMeter**: Generaci√≥n de carga y recolecci√≥n de m√©tricas
- **CloudWatch**: Monitoreo de instancias y servicios AWS
- **AWS CLI**: Verificaci√≥n de estado de Auto Scaling
- **Custom Scripts**: An√°lisis de logs y generaci√≥n de reportes

## 5. Resultados Esperados

### Escenario 1 - Carga Progresiva

| M√©trica | Esperado | Observado | Estado |
|---------|----------|-----------|--------|
| Tiempo Respuesta (p95) | < 2s | | |
| Tasa Error | < 1% | | |
| Instancias Activas (m√°x) | 3 | | |
| Tiempo Escalado | < 3 min | | |

### Escenario 2 - Carga Sostenida

| M√©trica | Esperado | Observado | Estado |
|---------|----------|-----------|--------|
| Tiempo Respuesta Promedio | < 1s | | |
| Tasa Error | < 0.5% | | |
| Instancias Activas | 1-2 | | |
| Variaci√≥n Latencia | < 10% | | |

## 6. An√°lisis de Resultados

### Escenario 1 - An√°lisis

[Completar con an√°lisis de resultados]

### Escenario 2 - An√°lisis

[Completar con an√°lisis de resultados]

## 7. Identificaci√≥n de Cuellos de Botella

[Completar con cuellos de botella identificados]

## 8. Recomendaciones para Escalabilidad Futura

### Corto Plazo (Pr√≥ximas 2-4 semanas)
- [Recomendaci√≥n 1]
- [Recomendaci√≥n 2]

### Mediano Plazo (1-3 meses)
- Implementar cach√© con ElastiCache (Redis)
- Optimizar consultas a base de datos
- Implementar CDN con CloudFront

### Largo Plazo (3-6 meses)
- Migrar a arquitectura de microservicios
- Implementar Kubernetes (EKS)
- Implementar disaster recovery multi-regi√≥n

## 9. Conclusiones

[Completar con conclusiones de las pruebas]

## 10. Anexos

### A. Configuraci√≥n de JMeter
[Detalles de configuraci√≥n]

### B. Logs de CloudWatch
[Referencias a logs relevantes]

### C. Gr√°ficas de Monitoreo
[Incluir capturas de CloudWatch]

---

**Documento preparado por**: [Nombres del equipo]
**Fecha de Entrega**: [Fecha]
**Versi√≥n Final**: 1.0

## Plan B
### Informe Resumido ‚Äî Rendimiento del Worker *(videos/min)*  
**Fecha:** 2025-11-09  

---

### üéØ Objetivo  
Evaluar el comportamiento del *worker* bajo condiciones de recursos limitados, identificando el punto de saturaci√≥n del sistema y el impacto en el throughput al incrementar la concurrencia en un entorno de c√≥mputo reducido.

---

### 1. Metodolog√≠a  

Se replic√≥ el procedimiento empleado en el **Escenario 2**, ejecutando pruebas controladas con **1** y **2** procesos concurrentes.  
La prueba con **4 workers** no pudo completarse debido a las restricciones de la instancia (**2 vCPU / 4 GB RAM**), que provocaban **fallas de estabilidad y reinicio del proceso Celery** al intentar superar ese umbral.  

Cada ejecuci√≥n mantuvo constantes el dataset, la configuraci√≥n de Redis y el procedimiento de encolamiento.  
Los *workers* se iniciaron con los siguientes comandos:

```bash
# Concurrencia 1
celery -A app.tasks.celery_app.celery_app worker -Q celery -n w1@%h -c 1 --loglevel=info

# Concurrencia 2
celery -A app.tasks.celery_app.celery_app worker -Q celery -n w1@%h -c 2 --loglevel=info
 ```

La carga de trabajo se gener√≥ con el mismo script de 11 tareas consecutivas:
```bash
python3 - <<'PY'
from app.tasks.video_tasks import process_video_task
video_id = "cc9318d6-b922-48c5-b71c-d927d3681a8f"
for _ in range(11):
process_video_task.delay(video_id)
print("‚úÖ Encoladas 11 tareas con √©xito")
PY
 ```
## üß© Resultados principales  

**Tabla de rendimiento ‚Äî Throughput promedio (videos/minuto):**  

<img width="1572" height="979" alt="image" src="https://github.com/user-attachments/assets/1ee338a3-4a65-4925-a1e6-1ca942301711" />


| Concurrencia | 50 MB | 100 MB |
|---------------|-------|--------|
| 1 worker | 1.9 | 0.8 |
| 2 workers | 3.4 | 1.7 |

> ‚ö†Ô∏è *El intento de ejecuci√≥n con 4 workers provoc√≥ un consumo total de CPU (>95 %) y RAM (>3.8 GB), ocasionando el cierre forzado del proceso Celery. No se obtuvieron m√©tricas v√°lidas para este caso.*

---

### üîç Hallazgos clave  

- El sistema mantiene estabilidad hasta **2 workers concurrentes**, pero no dispone de recursos suficientes para escalar m√°s all√°.  
- Se observa **ca√≠da del throughput (~30 %)** respecto al Escenario 2 debido al incremento de latencia en disco y la contenci√≥n de CPU.  
- Las tareas de **100 MB** presentan un **tiempo promedio de servicio casi doble**, producto de la decodificaci√≥n y el acceso a almacenamiento temporal.  
- Durante los picos de carga, la **cola de Redis crece sostenidamente**, aunque sin p√©rdida de mensajes.  

---

### üìä M√©tricas observadas  

| M√©trica | 50 MB | 100 MB |
|----------|--------|---------|
| Tiempo promedio por tarea | 29 s | 61 s |
| Uso de CPU | hasta 90 % | hasta 95 % |
| RAM | hasta 3.6 GB | hasta 3.9 GB |
| Error rate | 12‚Äì20 % (estable) | hasta 50 % bajo saturaci√≥n |

---

### ‚öôÔ∏è Recomendaciones  

- Mantener la concurrencia m√°xima en **2 workers por instancia** de estas caracter√≠sticas.  
- Evaluar el uso de una **instancia con ‚â• 4 vCPU y 8 GB RAM** para escenarios de alta carga.  
- Reducir operaciones de disco implementando **pre-carga en memoria (tmpfs)** o **cacheo local**.  
- Activar **`worker_prefetch_multiplier=1`** y la opci√≥n **`-Ofair`** para balancear la distribuci√≥n de tareas.  
- Incorporar **monitoreo en tiempo real con Prometheus/Grafana** para detectar saturaci√≥n temprana.  

---

### üß† Conclusi√≥n  

El sistema evidencia un **punto de saturaci√≥n temprano**: con **2 workers** alcanza el m√°ximo rendimiento sostenible (‚âà **3.4 videos/min para 50 MB**), mientras que cualquier intento de escalar m√°s all√° provoca inestabilidad y ca√≠da del servicio.  
A pesar de la reducci√≥n intencional de recursos, el *worker* mantiene un comportamiento controlado y confirma la **importancia de dimensionar la infraestructura** seg√∫n la carga esperada.



